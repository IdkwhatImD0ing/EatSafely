{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good and Bad Fruits Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Imports and Data Visualization\n",
    "Here, import the required libraries for this project. Furthermore, we define some constants. Furthermore, we load the data and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mimg\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import pandas as pd \n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import os, os.path\n",
    "import csv\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "for dirname, _, filenames in os.walk('dataset'): #See all files\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "batch_size = 32\n",
    "img_size = 256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how data is organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = []\n",
    "train_samples = []\n",
    "train_path = 'dataset/train/'\n",
    "\n",
    "for i in os.listdir(train_path):\n",
    "    train_categories.append(i)\n",
    "    train_samples.append(len(os.listdir(train_path + i)))\n",
    "\n",
    "test_samples = []\n",
    "test_path = 'dataset/test/'\n",
    "for i in os.listdir(test_path):\n",
    "    test_samples.append(test_path + i)\n",
    "\n",
    "print(\"Count of images in Training set:\", sum(train_samples))\n",
    "print(\"Count of images in Set set:\", len(test_samples))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing distributions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_size = plt.rcParams[\"figure.figsize\"]\n",
    "figure_size[0] = 40\n",
    "figure_size[1] = 20\n",
    "plt.rcParams[\"figure.figsize\"] = figure_size\n",
    "index = np.arange(len(train_categories))\n",
    "plt.bar(index, train_samples)\n",
    "plt.xlabel('Positions', fontsize=25)\n",
    "plt.ylabel('Count of Images', fontsize=25)\n",
    "plt.xticks(index, train_categories, fontsize=15, rotation=90)\n",
    "plt.title('Distrubution of Positions with counts in Training Set', fontsize=35)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will visualize some individual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ImageDataGenerator()\n",
    "dir_it = visualizer.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='categorical',\n",
    "    batch_size=5,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "x_batch, y_batch = next(dir_it)\n",
    "\n",
    "for i in range(5):\n",
    "    image = x_batch[i]\n",
    "    label = y_batch[i]\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the dataset is already augmented, it doesn't hurt to do some image augmentation ourselves.\n",
    "But since the images are already augmented, we will only do some mild augmentation.\n",
    "Next, we display the augmented images. Taking the first image for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "    )\n",
    "\n",
    "dir_it = visualizer.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_size, img_size),\n",
    "    class_mode='categorical',\n",
    "    batch_size=1,\n",
    "    seed = 42,\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    img, label = dir_it.next()\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(img[0])\n",
    "    plt.title(label[0])\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Creating the datasets\n",
    "\n",
    "Here we will be creating the dataset we will be using in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, batch_size, rescale, preprocess_function=None):\n",
    "    aug_gens = ImageDataGenerator(\n",
    "        rescale=rescale, # normalize pixel values to [0,1]\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        validation_split=0.1\n",
    "        preprocessing_function=preprocess_function\n",
    "    )\n",
    "\n",
    "    train_data = aug_gens.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        seed = 1447)\n",
    "\n",
    "    val_data = aug_gens.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        seed = 1447)\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "def get_testdataset(path, batch_size, rescale):\n",
    "    aug_gens = ImageDataGenerator(\n",
    "        rescale=rescale, # normalize pixel values to [0,1]\n",
    "        preprocessing_function=preprocess_function\n",
    "    )\n",
    "    \n",
    "    test_data = aug_gens.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(img_size, img_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        seed = 1447)\n",
    "\n",
    "    return test_data\n",
    "\n",
    "train, val = get_dataset('dataset/train', batch_size, 1./255)\n",
    "test = get_testdataset('dataset/test', batch_size, 1./255)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the dataset generator initialized correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.classes)\n",
    "print(test.classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three: Basic Model\n",
    "\n",
    "Here we will finally create our model!\n",
    "\n",
    "We will start off with a very basic model to ensure everything is working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=(img_size, img_size, 3)))\n",
    "model.add(\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides = (1,1), padding = \"same\", activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding = \"same\", activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding = \"same\", activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding = \"same\", activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1048, activation='swish'))\n",
    "model.add(keras.layers.Dense(128, activation='swish'))\n",
    "model.add(keras.layers.Dense(len(train_categories), activation='softmax'))\n",
    "\n",
    "model.build(input_shape=(None, img_size, img_size, 3))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Now for the fun part.\n",
    "We first start off by compiling the model with the Adam optimizer, and set the number of epochs to 10. Then we fit the model and display the accuracies in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "num_epochs = 10\n",
    "history = model.fit(train, workers = 8, epochs=num_epochs, validation_data=val, verbose = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Here, we will visualize the accuracy of the model as it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.1, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data\n",
    "Lets see how well our model does on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four: Advanced Model\n",
    "\n",
    "Here we will utilize transfer learning with a pretrained model. We will be using the MobileNet V2 Model since it is small, efficient, and fast. We will also redefine the input due to EfficientNet expecting different preprocessing requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = get_dataset('dataset/train/', batch_size, 1, preprocess_function=keras.applications.mobilenet_v2.preprocess_input)\n",
    "test = get_testdataset('dataset/test', batch_size, 1, preprocess_function=keras.applications.mobilenet_v2.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobNet = keras.applications.MobileNetV2(\n",
    "    input_shape=(img_size, img_size, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    "    pooling = 'avg'\n",
    ")\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(mobNet)\n",
    "model.add(keras.layers.Dense(1024, activation='swish'))\n",
    "model.add(keras.layers.Dense(128, activation='swish'))\n",
    "model.add(keras.layers.Dense(12, activation='softmax'))\n",
    "model.build(input_shape=(None, img_size, img_size, 3))\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will compile the model. However, this time we will add three callbacks. \n",
    " - The first call back will be a learning rate reduction. This will reduce the learning rate upon a plateau in loss to prevent overfitting. \n",
    " - The second callback will be an early stopping callback. This call back will allow us to use a high number for epochs and yet the model will stop once it detects that the improvements have stopped.\n",
    " - The third callback will be a model checkpoint. This will allow us to save the model at the end of each epoch. This will allow us to use the best model for testing.\n",
    "\n",
    "Once the three callbacks have been created, we will continue with training the model with the callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=4, verbose=1,  factor=0.4, min_lr=0.0001)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=8, mode='auto', restore_best_weights=True)\n",
    "\n",
    "MCP = keras.callbacks.ModelCheckpoint('Best_points.h5',\n",
    "                      verbose=1,\n",
    "                      save_best_only=True,\n",
    "                      monitor='val_accuracy',\n",
    "                      mode='max')\n",
    "\n",
    "history = model.fit(train,\n",
    "                    workers = 8,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_data=val,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose = 1,\n",
    "                    callbacks = [early_stop, lr_reduction, MCP])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will evaluate the accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
